{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2164de28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/peter/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/peter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize \n",
    "import string\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb114633",
   "metadata": {},
   "source": [
    "# Importing the text and slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "732fedc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data from data directory to one long raw string with no formatting\n",
    "\n",
    "pwd = !pwd\n",
    "path = pwd[0] + '/../data/alice_in_wonderland.txt'\n",
    "f = open(path, \"r\", newline = None)\n",
    "text = ' '.join(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a028d7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing the data before and after the specified beginning and ending, and removing chapter titles\n",
    "\n",
    "start = 'CHAPTER I. Down the Rabbit-Hole'\n",
    "end = 'THE END'\n",
    "start_ind = text.find(start)\n",
    "end_ind = text.find(end)\n",
    "novel = text[start_ind:end_ind+len(end)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2f58a",
   "metadata": {},
   "source": [
    "# Splitting the text by chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ad91c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the text into chapters and splitting these chapters into sentences for later use\n",
    "\n",
    "split_text = novel.split(\"CHAPTER\")\n",
    "chapter_list = []\n",
    "\n",
    "for chapter_text in split_text:\n",
    "    if chapter_text.strip():\n",
    "        chapter_list.append(\"CHAPTER\" + chapter_text)\n",
    "\n",
    "with open('../data/chapter_list.json', 'w') as json_file:\n",
    "    json.dump(chapter_list, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df41a712",
   "metadata": {},
   "source": [
    "# Pre-processing the text for Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "658decf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra cleaning to remove non-valid tokens after tokenizing\n",
    "\n",
    "def extra_token_cleaning(list_of_tokens):\n",
    "    '''Removing punctuation kept inside tokens aside from hyphens. Specifically aiming at words of the form '_very_' '''\n",
    "\n",
    "    index = string.punctuation.find('-')\n",
    "    punc_list = string.punctuation[:index] + string.punctuation[index+1:]\n",
    "\n",
    "    result = []\n",
    "    for token in list_of_tokens:\n",
    "        if any(char.isalpha() for char in token):\n",
    "            result.append(''.join([char for char in token if char.isalpha() or char == '-']))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3e5f87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing chapters and keeping strings\n",
    "\n",
    "chapters_tokenized = []\n",
    "\n",
    "for chapter in chapter_list:\n",
    "    chapters_tokenized.append(extra_token_cleaning(word_tokenize(chapter.lower())))\n",
    "\n",
    "with open('../data/chapters_tokenized.json', 'w') as json_file:\n",
    "    json.dump(chapters_tokenized, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57df541b",
   "metadata": {},
   "source": [
    "# Finding the complete and chapter-specific vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d64adf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a set to remove duplicates and saving vocabulary in an indexed list\n",
    "\n",
    "chapter_vocab = []\n",
    "for chapter in chapters_tokenized:\n",
    "    chapter_vocab.append(list(set((chapter))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74697547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating total vocab\n",
    "\n",
    "total_vocab = set([])\n",
    "for chapter in chapter_vocab:\n",
    "    total_vocab = total_vocab.union(set(chapter))\n",
    "    \n",
    "total_vocab = list(total_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f20b790",
   "metadata": {},
   "source": [
    "# Counting word occurrences and forming Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dde338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Dataframe and adding each chapter's respective vocabs\n",
    "\n",
    "df_info = []\n",
    "for i in range(0, 12):\n",
    "    for word in chapter_vocab[i]:\n",
    "        chapter_number = i+1\n",
    "        df_info.append((chapter_number, word))\n",
    "        \n",
    "columns = ['chapter_number', 'word']\n",
    "count_df = pd.DataFrame(df_info, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a67f63f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating custom count function \n",
    "\n",
    "def calculate_count(row):\n",
    "    chapter_number = row['chapter_number']\n",
    "    word = row['word']\n",
    "    chapter_words = chapters_tokenized[chapter_number-1]\n",
    "    number_of_word = chapter_words.count(word)\n",
    "    return int(number_of_word)\n",
    "\n",
    "count_df['count'] = count_df.apply(calculate_count, axis=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "038da99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking we haven't missed any words between the chapter list and the Dataframe\n",
    "\n",
    "total_words = []\n",
    "for chapter in chapters_tokenized:\n",
    "        for word in chapter:\n",
    "            total_words.append(word)\n",
    "total_word_length = len(total_words)\n",
    "\n",
    "remaining_word_list = total_words\n",
    "\n",
    "for index, row in count_df.iterrows():\n",
    "    count = row['count']\n",
    "    word = row['word']\n",
    "    for i in range(count):\n",
    "        remaining_word_list.remove(word)\n",
    "        \n",
    "assert(remaining_word_list == [])\n",
    "assert((count_df['count'].sum()) == total_word_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9420cb",
   "metadata": {},
   "source": [
    "# Saving the dataframe as a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9f8d1c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df.to_csv('../data/wordcount_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e4df39cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('../data/chapter_list.json', 'w') as json_file:\n",
    "    json.dump(chapter_list, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
